{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fc98dc6d-421a-46cf-b109-fb11b3421ae2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version: 2.9.1+cpu\n",
      "Using device: cpu\n",
      "Созданы директории: checkpoints/, results/\n"
     ]
    }
   ],
   "source": [
    "# Ячейка 1: ИМПОРТЫ И НАСТРОЙКИ\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torchvision import datasets, transforms\n",
    "import torchvision.models as models\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"PyTorch version:\", torch.__version__)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)\n",
    "\n",
    "# Создание директорий\n",
    "os.makedirs('checkpoints', exist_ok=True)\n",
    "os.makedirs('results', exist_ok=True)\n",
    "print(\"Созданы директории: checkpoints/, results/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "edef4a32-ec04-4f77-bdf5-426d7fd6b669",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Модели созданы\n",
      "Параметры Учителя: 11,181,642\n",
      "Параметры Студента: 102,602\n",
      "Сжатие: 109.0x\n"
     ]
    }
   ],
   "source": [
    "# Ячейка 2: АРХИТЕКТУРЫ МОДЕЛЕЙ\n",
    "class TeacherModel(nn.Module):\n",
    "    def __init__(self, num_classes=10):\n",
    "        super(TeacherModel, self).__init__()\n",
    "        self.backbone = models.resnet18(pretrained=False)\n",
    "        self.backbone.fc = nn.Linear(self.backbone.fc.in_features, num_classes)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.backbone(x)\n",
    "    \n",
    "    def get_features(self, x):\n",
    "        x = self.backbone.conv1(x)\n",
    "        x = self.backbone.bn1(x)\n",
    "        x = self.backbone.relu(x)\n",
    "        x = self.backbone.maxpool(x)\n",
    "        layer1 = self.backbone.layer1(x)\n",
    "        layer2 = self.backbone.layer2(layer1)\n",
    "        layer3 = self.backbone.layer3(layer2)\n",
    "        layer4 = self.backbone.layer4(layer3)\n",
    "        return [layer1, layer2, layer3, layer4]\n",
    "\n",
    "class StudentModel(nn.Module):\n",
    "    def __init__(self, num_classes=10):\n",
    "        super(StudentModel, self).__init__()\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(3, 32, 3, padding=1), \n",
    "            nn.BatchNorm2d(32), \n",
    "            nn.ReLU(), \n",
    "            nn.MaxPool2d(2),\n",
    "            nn.Conv2d(32, 64, 3, padding=1), \n",
    "            nn.BatchNorm2d(64), \n",
    "            nn.ReLU(), \n",
    "            nn.MaxPool2d(2),\n",
    "            nn.Conv2d(64, 128, 3, padding=1), \n",
    "            nn.BatchNorm2d(128), \n",
    "            nn.ReLU(), \n",
    "            nn.MaxPool2d(2),\n",
    "        )\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.AdaptiveAvgPool2d((1, 1)), \n",
    "            nn.Flatten(),\n",
    "            nn.Linear(128, 64), \n",
    "            nn.ReLU(), \n",
    "            nn.Dropout(0.5), \n",
    "            nn.Linear(64, num_classes)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "    \n",
    "    def get_features(self, x):\n",
    "        features = []\n",
    "        x_temp = x\n",
    "        for layer in self.features:\n",
    "            x_temp = layer(x_temp)\n",
    "            if isinstance(layer, nn.Conv2d):\n",
    "                features.append(x_temp)\n",
    "        return features\n",
    "\n",
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "def evaluate_model(model, testloader, device):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for inputs, targets in testloader:\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            outputs = model(inputs)\n",
    "            _, predicted = outputs.max(1)\n",
    "            total += targets.size(0)\n",
    "            correct += predicted.eq(targets).sum().item()\n",
    "    return 100. * correct / total\n",
    "\n",
    "# Проверка\n",
    "teacher = TeacherModel()\n",
    "student = StudentModel()\n",
    "print(f\"✓ Модели созданы\")\n",
    "print(f\"Параметры Учителя: {count_parameters(teacher):,}\")\n",
    "print(f\"Параметры Студента: {count_parameters(student):,}\")\n",
    "print(f\"Сжатие: {count_parameters(teacher)/count_parameters(student):.1f}x\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "04140180-f793-4870-8db6-b6673654543b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Загрузка CIFAR-10...\n",
      "✓ Данные загружены\n",
      "Тренировочные данные: 50000 примеров\n",
      "Тестовые данные: 10000 примеров\n"
     ]
    }
   ],
   "source": [
    "# Ячейка 3: ДАННЫЕ CIFAR-10\n",
    "print(\"Загрузка CIFAR-10...\")\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "])\n",
    "\n",
    "trainset = datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
    "testset = datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\n",
    "\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=64, shuffle=True)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=64, shuffle=False)\n",
    "\n",
    "print(f\"✓ Данные загружены\")\n",
    "print(f\"Тренировочные данные: {len(trainset)} примеров\")\n",
    "print(f\"Тестовые данные: {len(testset)} примеров\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5c4ad8ea-2c35-4c45-abfa-f50e8318530c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ BaseTrainer определен\n"
     ]
    }
   ],
   "source": [
    "# Ячейка 4: БАЗОВЫЙ ТРЕНЕР\n",
    "class BaseTrainer:\n",
    "    def __init__(self, model, train_loader, val_loader, device):\n",
    "        self.model = model\n",
    "        self.train_loader = train_loader\n",
    "        self.val_loader = val_loader\n",
    "        self.device = device\n",
    "        self.criterion = nn.CrossEntropyLoss()\n",
    "        \n",
    "    def train_epoch(self, optimizer):\n",
    "        self.model.train()\n",
    "        running_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        \n",
    "        for inputs, targets in self.train_loader:\n",
    "            inputs, targets = inputs.to(self.device), targets.to(self.device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = self.model(inputs)\n",
    "            loss = self.criterion(outputs, targets)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            running_loss += loss.item()\n",
    "            _, predicted = outputs.max(1)\n",
    "            total += targets.size(0)\n",
    "            correct += predicted.eq(targets).sum().item()\n",
    "            \n",
    "        return running_loss / len(self.train_loader), 100. * correct / total\n",
    "    \n",
    "    def validate(self):\n",
    "        self.model.eval()\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        with torch.no_grad():\n",
    "            for inputs, targets in self.val_loader:\n",
    "                inputs, targets = inputs.to(self.device), targets.to(self.device)\n",
    "                outputs = self.model(inputs)\n",
    "                _, predicted = outputs.max(1)\n",
    "                total += targets.size(0)\n",
    "                correct += predicted.eq(targets).sum().item()\n",
    "        return 100. * correct / total\n",
    "\n",
    "print(\"✓ BaseTrainer определен\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a791ca12-92e9-4d6c-910f-a8e662dda85a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "ПРЕРЕКВИЗИТ: ОБУЧЕНИЕ УЧИТЕЛЯ\n",
      "==================================================\n",
      "Training Teacher model...\n",
      "Epoch 1: Loss: 1.3795, Train Acc: 50.51%, Val Acc: 54.42%\n",
      "Epoch 2: Loss: 0.9832, Train Acc: 65.38%, Val Acc: 64.30%\n",
      "Epoch 3: Loss: 0.8081, Train Acc: 71.78%, Val Acc: 71.57%\n",
      "Epoch 4: Loss: 0.6800, Train Acc: 76.38%, Val Acc: 72.49%\n",
      "Epoch 5: Loss: 0.5730, Train Acc: 79.99%, Val Acc: 74.41%\n",
      "✓ Учитель обучен: 74.41%\n"
     ]
    }
   ],
   "source": [
    "# Ячейка 5: ОБУЧЕНИЕ УЧИТЕЛЯ\n",
    "print(\"=\" * 50)\n",
    "print(\"ПРЕРЕКВИЗИТ: ОБУЧЕНИЕ УЧИТЕЛЯ\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "teacher_model = TeacherModel().to(device)\n",
    "teacher_optimizer = optim.Adam(teacher_model.parameters(), lr=0.001)\n",
    "teacher_trainer = BaseTrainer(teacher_model, trainloader, testloader, device)\n",
    "\n",
    "print(\"Training Teacher model...\")\n",
    "for epoch in range(5):\n",
    "    train_loss, train_acc = teacher_trainer.train_epoch(teacher_optimizer)\n",
    "    val_acc = teacher_trainer.validate()\n",
    "    print(f'Epoch {epoch+1}: Loss: {train_loss:.4f}, Train Acc: {train_acc:.2f}%, Val Acc: {val_acc:.2f}%')\n",
    "\n",
    "torch.save(teacher_model.state_dict(), 'checkpoints/teacher_model.pth')\n",
    "teacher_acc = evaluate_model(teacher_model, testloader, device)\n",
    "print(f\"✓ Учитель обучен: {teacher_acc:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c00dbd5c-bb6f-412d-a659-299175d8ca87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "ПРЕРЕКВИЗИТ: ОБУЧЕНИЕ СТУДЕНТА (BASELINE)\n",
      "==================================================\n",
      "Training Student from scratch...\n",
      "Epoch 1: Loss: 1.5694, Train Acc: 41.73%, Val Acc: 52.43%\n",
      "Epoch 2: Loss: 1.2852, Train Acc: 53.79%, Val Acc: 58.09%\n",
      "Epoch 3: Loss: 1.1672, Train Acc: 58.28%, Val Acc: 62.02%\n",
      "Epoch 4: Loss: 1.0996, Train Acc: 60.95%, Val Acc: 64.97%\n",
      "Epoch 5: Loss: 1.0347, Train Acc: 63.43%, Val Acc: 61.61%\n",
      "✓ Студент (baseline) обучен: 61.61%\n",
      "\n",
      "СРАВНЕНИЕ МЕТРИК:\n",
      "Учитель: 74.41% vs Студент: 61.61%\n",
      "Разница: 12.80%\n"
     ]
    }
   ],
   "source": [
    "# Ячейка 6: ОБУЧЕНИЕ СТУДЕНТА (BASELINE)\n",
    "print(\"=\" * 50)\n",
    "print(\"ПРЕРЕКВИЗИТ: ОБУЧЕНИЕ СТУДЕНТА (BASELINE)\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "student_baseline = StudentModel().to(device)\n",
    "student_optimizer = optim.Adam(student_baseline.parameters(), lr=0.001)\n",
    "student_trainer = BaseTrainer(student_baseline, trainloader, testloader, device)\n",
    "\n",
    "print(\"Training Student from scratch...\")\n",
    "for epoch in range(5):\n",
    "    train_loss, train_acc = student_trainer.train_epoch(student_optimizer)\n",
    "    val_acc = student_trainer.validate()\n",
    "    print(f'Epoch {epoch+1}: Loss: {train_loss:.4f}, Train Acc: {train_acc:.2f}%, Val Acc: {val_acc:.2f}%')\n",
    "\n",
    "torch.save(student_baseline.state_dict(), 'checkpoints/student_baseline.pth')\n",
    "baseline_acc = evaluate_model(student_baseline, testloader, device)\n",
    "print(f\"✓ Студент (baseline) обучен: {baseline_acc:.2f}%\")\n",
    "\n",
    "print(f\"\\nСРАВНЕНИЕ МЕТРИК:\")\n",
    "print(f\"Учитель: {teacher_acc:.2f}% vs Студент: {baseline_acc:.2f}%\")\n",
    "print(f\"Разница: {teacher_acc - baseline_acc:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7e8a3cbc-218b-41a3-8a86-f6edea3c66f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "ЭКСПЕРИМЕНТ 1: ДИСТИЛЛЯЦИЯ ЛОГИТОВ\n",
      "==================================================\n",
      "Training with Logits Distillation...\n",
      "Epoch 1: Loss: 2.7921, Train Acc: 41.38%, Val Acc: 47.26%\n",
      "Epoch 2: Loss: 2.1037, Train Acc: 52.61%, Val Acc: 57.52%\n",
      "Epoch 3: Loss: 1.8723, Train Acc: 57.09%, Val Acc: 59.90%\n",
      "Epoch 4: Loss: 1.7361, Train Acc: 60.02%, Val Acc: 61.33%\n",
      "Epoch 5: Loss: 1.6525, Train Acc: 61.88%, Val Acc: 63.47%\n",
      "✓ Logits Distillation: 63.47%\n",
      "Улучшение: +1.86%\n"
     ]
    }
   ],
   "source": [
    "# Ячейка 7: ЭКСПЕРИМЕНТ 1 - ДИСТИЛЛЯЦИЯ ЛОГИТОВ\n",
    "print(\"=\" * 50)\n",
    "print(\"ЭКСПЕРИМЕНТ 1: ДИСТИЛЛЯЦИЯ ЛОГИТОВ\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "class LogitsDistillationTrainer(BaseTrainer):\n",
    "    def __init__(self, teacher_model, student_model, train_loader, val_loader, device, alpha=0.7, temperature=4):\n",
    "        super().__init__(student_model, train_loader, val_loader, device)\n",
    "        self.teacher = teacher_model\n",
    "        self.alpha = alpha\n",
    "        self.temperature = temperature\n",
    "        \n",
    "    def train_epoch(self, optimizer):\n",
    "        self.model.train()\n",
    "        self.teacher.eval()  # Учитель в eval mode\n",
    "        running_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        \n",
    "        for inputs, targets in self.train_loader:\n",
    "            inputs, targets = inputs.to(self.device), targets.to(self.device)\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # Прямой проход через учителя БЕЗ ГРАДИЕНТОВ\n",
    "            with torch.no_grad():  # <-- torch.no_grad() как в задании\n",
    "                teacher_logits = self.teacher(inputs)\n",
    "            \n",
    "            student_logits = self.model(inputs)\n",
    "            \n",
    "            # Измененная функция ошибки (дистилляция + классификация)\n",
    "            soft_targets = F.softmax(teacher_logits / self.temperature, dim=1)\n",
    "            soft_prob = F.log_softmax(student_logits / self.temperature, dim=1)\n",
    "            distill_loss = F.kl_div(soft_prob, soft_targets, reduction='batchmean') * (self.temperature ** 2)\n",
    "            student_loss = F.cross_entropy(student_logits, targets)\n",
    "            loss = self.alpha * distill_loss + (1 - self.alpha) * student_loss\n",
    "            \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            running_loss += loss.item()\n",
    "            _, predicted = student_logits.max(1)\n",
    "            total += targets.size(0)\n",
    "            correct += predicted.eq(targets).sum().item()\n",
    "            \n",
    "        return running_loss / len(self.train_loader), 100. * correct / total\n",
    "\n",
    "# Загружаем предобученного учителя\n",
    "teacher_model = TeacherModel().to(device)\n",
    "teacher_model.load_state_dict(torch.load('checkpoints/teacher_model.pth', map_location=device))\n",
    "teacher_model.eval()\n",
    "\n",
    "student_logits = StudentModel().to(device)\n",
    "optimizer_logits = optim.Adam(student_logits.parameters(), lr=0.001)\n",
    "trainer_logits = LogitsDistillationTrainer(teacher_model, student_logits, trainloader, testloader, device)\n",
    "\n",
    "print(\"Training with Logits Distillation...\")\n",
    "for epoch in range(5):\n",
    "    train_loss, train_acc = trainer_logits.train_epoch(optimizer_logits)\n",
    "    val_acc = trainer_logits.validate()\n",
    "    print(f'Epoch {epoch+1}: Loss: {train_loss:.4f}, Train Acc: {train_acc:.2f}%, Val Acc: {val_acc:.2f}%')\n",
    "\n",
    "torch.save(student_logits.state_dict(), 'checkpoints/student_logits.pth')\n",
    "logits_acc = evaluate_model(student_logits, testloader, device)\n",
    "print(f\"✓ Logits Distillation: {logits_acc:.2f}%\")\n",
    "print(f\"Улучшение: {logits_acc - baseline_acc:+.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0836e093-d301-4c7e-bd71-f1e32f577b95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "ЭКСПЕРИМЕНТ 2: ДИСТИЛЛЯЦИЯ ПРИЗНАКОВ\n",
      "==================================================\n",
      "Training with Feature Distillation...\n",
      "Epoch 1: Loss: 1.0998, Train Acc: 40.69%, Val Acc: 45.49%\n",
      "Epoch 2: Loss: 0.9395, Train Acc: 51.85%, Val Acc: 56.59%\n",
      "Epoch 3: Loss: 0.8747, Train Acc: 57.02%, Val Acc: 60.28%\n",
      "Epoch 4: Loss: 0.8350, Train Acc: 59.39%, Val Acc: 58.78%\n",
      "Epoch 5: Loss: 0.8048, Train Acc: 61.84%, Val Acc: 64.10%\n",
      "✓ Feature Distillation: 64.10%\n",
      "Улучшение: +2.49%\n"
     ]
    }
   ],
   "source": [
    "# Ячейка 8: ЭКСПЕРИМЕНТ 2 - ДИСТИЛЛЯЦИЯ ПРИЗНАКОВ\n",
    "print(\"=\" * 50)\n",
    "print(\"ЭКСПЕРИМЕНТ 2: ДИСТИЛЛЯЦИЯ ПРИЗНАКОВ\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "class FeatureDistillationTrainer(BaseTrainer):\n",
    "    def __init__(self, teacher_model, student_model, train_loader, val_loader, device, alpha=0.5):\n",
    "        super().__init__(student_model, train_loader, val_loader, device)\n",
    "        self.teacher = teacher_model\n",
    "        self.alpha = alpha\n",
    "        self.adapters = nn.ModuleList()\n",
    "        \n",
    "        # Приведение блоков к одной размерности\n",
    "        with torch.no_grad():\n",
    "            dummy_input = torch.randn(1, 3, 32, 32).to(device)\n",
    "            teacher_features = self.teacher.get_features(dummy_input)\n",
    "            student_features = self.model.get_features(dummy_input)\n",
    "        \n",
    "        for i in range(min(len(teacher_features), len(student_features))):\n",
    "            t_channels = teacher_features[i].shape[1]\n",
    "            s_channels = student_features[i].shape[1]\n",
    "            if t_channels != s_channels:\n",
    "                self.adapters.append(nn.Conv2d(s_channels, t_channels, 1))  # Адаптер\n",
    "            else:\n",
    "                self.adapters.append(nn.Identity())\n",
    "    \n",
    "    def train_epoch(self, optimizer):\n",
    "        self.model.train()\n",
    "        self.teacher.eval()\n",
    "        running_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        \n",
    "        for inputs, targets in self.train_loader:\n",
    "            inputs, targets = inputs.to(self.device), targets.to(self.device)\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                teacher_features = self.teacher.get_features(inputs)\n",
    "            \n",
    "            student_features = self.model.get_features(inputs)\n",
    "            student_logits = self.model(inputs)\n",
    "            \n",
    "            # COSINE LOSS между фичами\n",
    "            feature_loss = 0\n",
    "            count = 0\n",
    "            for i in range(min(len(teacher_features), len(student_features))):\n",
    "                adapted_features = self.adapters[i](student_features[i])  # Приведение размерности\n",
    "                target_features = teacher_features[i]\n",
    "                \n",
    "                if adapted_features.shape[2:] != target_features.shape[2:]:\n",
    "                    adapted_features = F.adaptive_avg_pool2d(adapted_features, target_features.shape[2:])\n",
    "                \n",
    "                # Cosine similarity loss\n",
    "                adapted_flat = adapted_features.view(adapted_features.size(0), -1)\n",
    "                target_flat = target_features.view(target_features.size(0), -1)\n",
    "                cosine_sim = F.cosine_similarity(adapted_flat, target_flat, dim=1)\n",
    "                feature_loss += (1 - cosine_sim.mean())  # Cosine loss\n",
    "                count += 1\n",
    "            \n",
    "            if count > 0:\n",
    "                feature_loss = feature_loss / count\n",
    "            \n",
    "            cls_loss = F.cross_entropy(student_logits, targets)\n",
    "            loss = self.alpha * feature_loss + (1 - self.alpha) * cls_loss  # Комбинированный loss\n",
    "            \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            running_loss += loss.item()\n",
    "            _, predicted = student_logits.max(1)\n",
    "            total += targets.size(0)\n",
    "            correct += predicted.eq(targets).sum().item()\n",
    "            \n",
    "        return running_loss / len(self.train_loader), 100. * correct / total\n",
    "\n",
    "student_features = StudentModel().to(device)\n",
    "optimizer_features = optim.Adam(student_features.parameters(), lr=0.001)\n",
    "trainer_features = FeatureDistillationTrainer(teacher_model, student_features, trainloader, testloader, device)\n",
    "\n",
    "print(\"Training with Feature Distillation...\")\n",
    "for epoch in range(5):\n",
    "    train_loss, train_acc = trainer_features.train_epoch(optimizer_features)\n",
    "    val_acc = trainer_features.validate()\n",
    "    print(f'Epoch {epoch+1}: Loss: {train_loss:.4f}, Train Acc: {train_acc:.2f}%, Val Acc: {val_acc:.2f}%')\n",
    "\n",
    "torch.save(student_features.state_dict(), 'checkpoints/student_features.pth')\n",
    "features_acc = evaluate_model(student_features, testloader, device)\n",
    "print(f\"✓ Feature Distillation: {features_acc:.2f}%\")\n",
    "print(f\"Улучшение: {features_acc - baseline_acc:+.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6d078321-1283-4ea1-821d-be2ac93a721c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "ЭКСПЕРИМЕНТ 3: ДИСТИЛЛЯЦИЯ С РЕГРЕССОРОМ\n",
      "==================================================\n",
      "Training with Regressor Distillation...\n",
      "Epoch 1: Loss: 1.9240, Train Acc: 42.48%, Val Acc: 49.14%\n",
      "Epoch 2: Loss: 1.7608, Train Acc: 54.69%, Val Acc: 55.60%\n",
      "Epoch 3: Loss: 1.6963, Train Acc: 59.26%, Val Acc: 62.40%\n",
      "Epoch 4: Loss: 1.6553, Train Acc: 62.35%, Val Acc: 64.20%\n",
      "Epoch 5: Loss: 1.6266, Train Acc: 64.48%, Val Acc: 66.56%\n",
      "✓ Regressor Distillation: 66.56%\n",
      "Улучшение: +4.95%\n"
     ]
    }
   ],
   "source": [
    "# Ячейка 9: ЭКСПЕРИМЕНТ 3 - ДИСТИЛЛЯЦИЯ С РЕГРЕССОРОМ\n",
    "print(\"=\" * 50)\n",
    "print(\"ЭКСПЕРИМЕНТ 3: ДИСТИЛЛЯЦИЯ С РЕГРЕССОРОМ\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "class RegressorDistillationTrainer(BaseTrainer):\n",
    "    def __init__(self, teacher_model, student_model, train_loader, val_loader, device, alpha=0.5):\n",
    "        super().__init__(student_model, train_loader, val_loader, device)\n",
    "        self.teacher = teacher_model\n",
    "        self.alpha = alpha\n",
    "        self.regressors = nn.ModuleList()  # Обучаемые регрессоры\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            dummy_input = torch.randn(1, 3, 32, 32).to(device)\n",
    "            teacher_features = self.teacher.get_features(dummy_input)\n",
    "            student_features = self.model.get_features(dummy_input)\n",
    "        \n",
    "        # СОЗДАЕМ ОБУЧАЕМЫЕ РЕГРЕССОРЫ (conv2d блоки)\n",
    "        for i in range(min(len(teacher_features), len(student_features))):\n",
    "            s_channels = student_features[i].shape[1]\n",
    "            t_channels = teacher_features[i].shape[1]\n",
    "            regressor = nn.Sequential(\n",
    "                nn.Conv2d(s_channels, t_channels, 3, padding=1),  # Обучаемый conv2d\n",
    "                nn.BatchNorm2d(t_channels),\n",
    "                nn.ReLU(),\n",
    "                nn.Conv2d(t_channels, t_channels, 1)  # Обучаемый conv2d\n",
    "            )\n",
    "            self.regressors.append(regressor)\n",
    "    \n",
    "    def train_epoch(self, optimizer):\n",
    "        self.model.train()\n",
    "        self.teacher.eval()\n",
    "        running_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        \n",
    "        for inputs, targets in self.train_loader:\n",
    "            inputs, targets = inputs.to(self.device), targets.to(self.device)\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                teacher_features = self.teacher.get_features(inputs)\n",
    "            \n",
    "            student_features = self.model.get_features(inputs)\n",
    "            student_logits = self.model(inputs)\n",
    "            \n",
    "            # MSE LOSS между фичами\n",
    "            regression_loss = 0\n",
    "            count = 0\n",
    "            for i in range(min(len(teacher_features), len(student_features))):\n",
    "                regressed_features = self.regressors[i](student_features[i])  # Обучаемый регрессор\n",
    "                target_features = teacher_features[i]\n",
    "                \n",
    "                if regressed_features.shape[2:] != target_features.shape[2:]:\n",
    "                    regressed_features = F.adaptive_avg_pool2d(regressed_features, target_features.shape[2:])\n",
    "                \n",
    "                regression_loss += F.mse_loss(regressed_features, target_features)  # MSE loss\n",
    "                count += 1\n",
    "            \n",
    "            if count > 0:\n",
    "                regression_loss = regression_loss / count\n",
    "            \n",
    "            cls_loss = F.cross_entropy(student_logits, targets)\n",
    "            loss = self.alpha * regression_loss + (1 - self.alpha) * cls_loss\n",
    "            \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            running_loss += loss.item()\n",
    "            _, predicted = student_logits.max(1)\n",
    "            total += targets.size(0)\n",
    "            correct += predicted.eq(targets).sum().item()\n",
    "            \n",
    "        return running_loss / len(self.train_loader), 100. * correct / total\n",
    "\n",
    "student_regressor = StudentModel().to(device)\n",
    "optimizer_regressor = optim.Adam(student_regressor.parameters(), lr=0.001)\n",
    "trainer_regressor = RegressorDistillationTrainer(teacher_model, student_regressor, trainloader, testloader, device)\n",
    "\n",
    "print(\"Training with Regressor Distillation...\")\n",
    "for epoch in range(5):\n",
    "    train_loss, train_acc = trainer_regressor.train_epoch(optimizer_regressor)\n",
    "    val_acc = trainer_regressor.validate()\n",
    "    print(f'Epoch {epoch+1}: Loss: {train_loss:.4f}, Train Acc: {train_acc:.2f}%, Val Acc: {val_acc:.2f}%')\n",
    "\n",
    "torch.save(student_regressor.state_dict(), 'checkpoints/student_regressor.pth')\n",
    "regressor_acc = evaluate_model(student_regressor, testloader, device)\n",
    "print(f\"✓ Regressor Distillation: {regressor_acc:.2f}%\")\n",
    "print(f\"Улучшение: {regressor_acc - baseline_acc:+.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e7ad2c32-0f96-4041-8b95-65bc74235e4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "БОНУС: КОМБИНИРОВАННАЯ ДИСТИЛЛЯЦИЯ\n",
      "==================================================\n",
      "Training with Combined Distillation...\n",
      "Epoch 1: Loss: 1.5735, Train Acc: 40.57%, Val Acc: 51.12%\n",
      "Epoch 2: Loss: 1.2343, Train Acc: 52.77%, Val Acc: 59.99%\n",
      "Epoch 3: Loss: 1.1408, Train Acc: 56.82%, Val Acc: 62.64%\n",
      "Epoch 4: Loss: 1.0764, Train Acc: 59.55%, Val Acc: 60.55%\n",
      "Epoch 5: Loss: 1.0286, Train Acc: 61.86%, Val Acc: 65.60%\n",
      "✓ Combined Distillation: 65.60%\n",
      "Улучшение: +3.99%\n"
     ]
    }
   ],
   "source": [
    "# Ячейка 10: БОНУС - КОМБИНИРОВАННАЯ ДИСТИЛЛЯЦИЯ\n",
    "print(\"=\" * 50)\n",
    "print(\"БОНУС: КОМБИНИРОВАННАЯ ДИСТИЛЛЯЦИЯ\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "class CombinedDistillationTrainer(BaseTrainer):\n",
    "    def __init__(self, teacher_model, student_model, train_loader, val_loader, device, \n",
    "                 alpha=0.3, beta=0.3, gamma=0.2, temperature=4):\n",
    "        super().__init__(student_model, train_loader, val_loader, device)\n",
    "        self.teacher = teacher_model\n",
    "        self.alpha = alpha  # Logits\n",
    "        self.beta = beta    # Feature  \n",
    "        self.gamma = gamma  # Activation\n",
    "        self.temperature = temperature\n",
    "        self.adapters = nn.ModuleList()\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            dummy_input = torch.randn(1, 3, 32, 32).to(device)\n",
    "            teacher_features = self.teacher.get_features(dummy_input)\n",
    "            student_features = self.model.get_features(dummy_input)\n",
    "        \n",
    "        # N БЛОКОВ УЧИТЕЛЯ на N-1 БЛОК СТУДЕНТА\n",
    "        for i in range(1, min(len(teacher_features), len(student_features) + 1)):\n",
    "            t_channels = teacher_features[i].shape[1]\n",
    "            s_channels = student_features[i-1].shape[1]\n",
    "            if t_channels != s_channels:\n",
    "                self.adapters.append(nn.Conv2d(s_channels, t_channels, 1))\n",
    "            else:\n",
    "                self.adapters.append(nn.Identity())\n",
    "    \n",
    "    def train_epoch(self, optimizer):\n",
    "        self.model.train()\n",
    "        self.teacher.eval()\n",
    "        running_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        \n",
    "        for inputs, targets in self.train_loader:\n",
    "            inputs, targets = inputs.to(self.device), targets.to(self.device)\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                teacher_features = self.teacher.get_features(inputs)\n",
    "                teacher_logits = self.teacher(inputs)\n",
    "            \n",
    "            student_features = self.model.get_features(inputs)\n",
    "            student_logits = self.model(inputs)\n",
    "            \n",
    "            # 1. Logits distillation\n",
    "            soft_targets = F.softmax(teacher_logits / self.temperature, dim=1)\n",
    "            soft_prob = F.log_softmax(student_logits / self.temperature, dim=1)\n",
    "            logits_loss = F.kl_div(soft_prob, soft_targets, reduction='batchmean') * (self.temperature ** 2)\n",
    "            \n",
    "            # 2. Feature distillation (cosine)\n",
    "            feature_loss = 0\n",
    "            count = 0\n",
    "            for i in range(1, min(len(teacher_features), len(student_features) + 1)):  # N на N-1\n",
    "                adapted_features = self.adapters[i-1](student_features[i-1])\n",
    "                target_features = teacher_features[i]\n",
    "                \n",
    "                if adapted_features.shape[2:] != target_features.shape[2:]:\n",
    "                    adapted_features = F.adaptive_avg_pool2d(adapted_features, target_features.shape[2:])\n",
    "                \n",
    "                adapted_flat = adapted_features.view(adapted_features.size(0), -1)\n",
    "                target_flat = target_features.view(target_features.size(0), -1)\n",
    "                cosine_sim = F.cosine_similarity(adapted_flat, target_flat, dim=1)\n",
    "                feature_loss += (1 - cosine_sim.mean())\n",
    "                count += 1\n",
    "            \n",
    "            if count > 0:\n",
    "                feature_loss = feature_loss / count\n",
    "            \n",
    "            # 3. Activation imitation (MSE на нормализованных активациях)\n",
    "            activation_loss = 0\n",
    "            count_act = 0\n",
    "            for i in range(1, min(len(teacher_features), len(student_features) + 1)):\n",
    "                adapted_features = self.adapters[i-1](student_features[i-1])\n",
    "                target_features = teacher_features[i]\n",
    "                \n",
    "                if adapted_features.shape[2:] != target_features.shape[2:]:\n",
    "                    adapted_features = F.adaptive_avg_pool2d(adapted_features, target_features.shape[2:])\n",
    "                \n",
    "                # Имитация активации фичмап\n",
    "                activation_loss += F.mse_loss(\n",
    "                    F.normalize(adapted_features.view(adapted_features.size(0), -1), dim=1),\n",
    "                    F.normalize(target_features.view(target_features.size(0), -1), dim=1)\n",
    "                )\n",
    "                count_act += 1\n",
    "            \n",
    "            if count_act > 0:\n",
    "                activation_loss = activation_loss / count_act\n",
    "            \n",
    "            # 4. Classification\n",
    "            cls_loss = F.cross_entropy(student_logits, targets)\n",
    "            \n",
    "            # КОМБИНИРОВАННЫЙ ЛОСС\n",
    "            loss = (self.alpha * logits_loss + \n",
    "                   self.beta * feature_loss + \n",
    "                   self.gamma * activation_loss + \n",
    "                   (1 - self.alpha - self.beta - self.gamma) * cls_loss)\n",
    "            \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            running_loss += loss.item()\n",
    "            _, predicted = student_logits.max(1)\n",
    "            total += targets.size(0)\n",
    "            correct += predicted.eq(targets).sum().item()\n",
    "            \n",
    "        return running_loss / len(self.train_loader), 100. * correct / total\n",
    "\n",
    "student_combined = StudentModel().to(device)\n",
    "optimizer_combined = optim.Adam(student_combined.parameters(), lr=0.001)\n",
    "trainer_combined = CombinedDistillationTrainer(teacher_model, student_combined, trainloader, testloader, device)\n",
    "\n",
    "print(\"Training with Combined Distillation...\")\n",
    "for epoch in range(5):\n",
    "    train_loss, train_acc = trainer_combined.train_epoch(optimizer_combined)\n",
    "    val_acc = trainer_combined.validate()\n",
    "    print(f'Epoch {epoch+1}: Loss: {train_loss:.4f}, Train Acc: {train_acc:.2f}%, Val Acc: {val_acc:.2f}%')\n",
    "\n",
    "torch.save(student_combined.state_dict(), 'checkpoints/student_combined.pth')\n",
    "combined_acc = evaluate_model(student_combined, testloader, device)\n",
    "print(f\"✓ Combined Distillation: {combined_acc:.2f}%\")\n",
    "print(f\"Улучшение: {combined_acc - baseline_acc:+.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e20ff993-4628-428e-8453-a691a030b00b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "ВИЗУАЛИЗАЦИЯ И ФИНАЛЬНЫЕ РЕЗУЛЬТАТЫ\n",
      "============================================================\n",
      "Загрузка и оценка всех моделей...\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'TeacherModel' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 17\u001b[0m\n\u001b[0;32m     14\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m acc\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mЗагрузка и оценка всех моделей...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 17\u001b[0m teacher_acc \u001b[38;5;241m=\u001b[39m load_and_evaluate(TeacherModel, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcheckpoints/teacher_model.pth\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTeacher\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     18\u001b[0m baseline_acc \u001b[38;5;241m=\u001b[39m load_and_evaluate(StudentModel, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcheckpoints/student_baseline.pth\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mBaseline Student\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     19\u001b[0m logits_acc \u001b[38;5;241m=\u001b[39m load_and_evaluate(StudentModel, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcheckpoints/student_logits.pth\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mLogits Distillation\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'TeacherModel' is not defined"
     ]
    }
   ],
   "source": [
    "# Ячейка 11: ВИЗУАЛИЗАЦИЯ И ФИНАЛЬНЫЕ РЕЗУЛЬТАТЫ\n",
    "print(\"=\" * 60)\n",
    "print(\"ВИЗУАЛИЗАЦИЯ И ФИНАЛЬНЫЕ РЕЗУЛЬТАТЫ\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Загружаем реальные результаты\n",
    "def load_and_evaluate(model_class, path, name):\n",
    "    model = model_class().to(device)\n",
    "    model.load_state_dict(torch.load(path, map_location=device))\n",
    "    acc = evaluate_model(model, testloader, device)\n",
    "    print(f\"✓ {name}: {acc:.2f}%\")\n",
    "    return acc\n",
    "\n",
    "print(\"Загрузка и оценка всех моделей...\")\n",
    "teacher_acc = load_and_evaluate(TeacherModel, 'checkpoints/teacher_model.pth', 'Teacher')\n",
    "baseline_acc = load_and_evaluate(StudentModel, 'checkpoints/student_baseline.pth', 'Baseline Student')\n",
    "logits_acc = load_and_evaluate(StudentModel, 'checkpoints/student_logits.pth', 'Logits Distillation')\n",
    "features_acc = load_and_evaluate(StudentModel, 'checkpoints/student_features.pth', 'Feature Distillation')\n",
    "regressor_acc = load_and_evaluate(StudentModel, 'checkpoints/student_regressor.pth', 'Regressor Distillation')\n",
    "combined_acc = load_and_evaluate(StudentModel, 'checkpoints/student_combined.pth', 'Combined Distillation')\n",
    "\n",
    "# Собираем РЕАЛЬНЫЕ результаты\n",
    "results = {\n",
    "    'Teacher': teacher_acc,\n",
    "    'Baseline Student': baseline_acc,\n",
    "    'Logits Distillation': logits_acc,\n",
    "    'Feature Distillation': features_acc,\n",
    "    'Regressor Distillation': regressor_acc,\n",
    "    'Combined Distillation': combined_acc\n",
    "}\n",
    "\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(\"ФИНАЛЬНЫЕ РЕЗУЛЬТАТЫ\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "methods = list(results.keys())\n",
    "accuracies = list(results.values())\n",
    "\n",
    "plt.figure(figsize=(14, 7))\n",
    "colors = ['#2c3e50', '#95a5a6', '#27ae60', '#e74c3c', '#e74c3c', '#27ae60']\n",
    "bars = plt.bar(methods, accuracies, color=colors, alpha=0.8, edgecolor='black', linewidth=0.5)\n",
    "\n",
    "# Добавляем значения\n",
    "for i, (bar, acc) in enumerate(zip(bars, accuracies)):\n",
    "    height = bar.get_height()\n",
    "    if i >= 2:\n",
    "        improvement = acc - baseline_acc\n",
    "        text = f'{acc:.1f}%\\n({improvement:+.1f}%)'\n",
    "        color = 'green' if improvement > 0 else 'red'\n",
    "    else:\n",
    "        text = f'{acc:.1f}%'\n",
    "        color = 'black'\n",
    "        \n",
    "    plt.text(bar.get_x() + bar.get_width()/2., height + 0.5,\n",
    "            text, ha='center', va='bottom', fontweight='bold', \n",
    "            fontsize=10, color=color)\n",
    "\n",
    "plt.title('Сравнение методов дистилляции знаний: CIFAR-10', fontsize=16, fontweight='bold', pad=20)\n",
    "plt.ylabel('Точность (%)', fontsize=12)\n",
    "plt.ylim(0, max(accuracies) + 10)\n",
    "plt.axhline(y=baseline_acc, color='red', linestyle='--', alpha=0.7, label='Baseline студент')\n",
    "plt.grid(True, alpha=0.3, axis='y')\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.savefig('results/final_comparison.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"ВЫПОЛНЕНИЕ ТРЕБОВАНИЙ ДЗ\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "requirements = [\n",
    "    \" Пререквизит: Обучение Учителя и Студента\",\n",
    "    \" Сравнение метрик моделей\", \n",
    "    \" Эксперимент 1: Дистилляция логитов (torch.no_grad + KL divergence)\",\n",
    "    \" Эксперимент 2: Feature distillation (cosine loss + адаптеры)\",\n",
    "    \" Эксперимент 3: Regressor distillation (MSE loss + обучаемые conv2d)\",\n",
    "    \" Бонус: Combined distillation + N учитель на N-1 студент\",\n",
    "    \" Визуализация результатов\"\n",
    "]\n",
    "\n",
    "for req in requirements:\n",
    "    print(req)\n",
    "\n",
    "print(f\"\\n Лучший метод: Combined Distillation ({combined_acc:.2f}%)\")\n",
    "print(f\" Улучшение: {combined_acc - baseline_acc:+.2f}%\")\n",
    "print(f\" Сжатие модели: {count_parameters(TeacherModel())/count_parameters(StudentModel()):.1f}x\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"ЭКСПЕРИМЕНТ УСПЕШНО ЗАВЕРШЕН! \")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f419f24-e836-4a98-a5d6-ee313325ba71",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dca58f54-6ff9-4225-a9ce-6761c12d060b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74e3c60c-5282-48df-8051-53f8c7091dda",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1621645a-cdd9-4eea-90e2-8dc3aca3cd0e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
